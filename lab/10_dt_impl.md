# Lab 6: Decision Trees

1. Implement a program in python that computes the Gini impurity of each feature for a given dataset with 3 features.  Use the example dataset discussed in the class.
   <table align="center">
     <tr>       <th>Loves Popcorn</th>       <th>Loves Soda</th>       <th>Age</td>       <th>Loves Icecream</th>     </tr>
     <tr>       <td>Yes</td>       <td>Yes</td>       <td>7</td>       <td>No</td>     </tr>
     <tr>       <td>Yes</td>       <td>No</td>       <td>12</td>       <td>No</td>     </tr>
     <tr>       <td>No</td>       <td>Yes</td>       <td>18</td>       <td>Yes</td>     </tr>
     <tr>       <td>No</td>       <td>Yes</td>       <td>35</td>       <td>Yes</td>     </tr>
     <tr>       <td>Yes</td>       <td>Yes</td>       <td>38</td>       <td>Yes</td>     </tr>
     <tr>       <td>Yes</td>       <td>No</td>       <td>50</td>       <td>No</td>     </tr>
     <tr>       <td>No</td>       <td>No</td>       <td>83</td>       <td>No</td>     </tr>
   </table>
   
2. Generate from the scratch implementation of decision tree (based on Gini impurity) and understand the code.
3. Generate an implementation that uses Entropy as opposed to Gini impurity.
4. Check if the classification for a test data is same for both.
5. For a downloaded data set, try the decision tree program and see it working.
6. Use Scikit-learn's Decision Tree implementation on the above downloaded data set.
7. Use Tensor Flow's Decision Tree implementation on the above downloaded data set.
